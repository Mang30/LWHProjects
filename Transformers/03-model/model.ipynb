{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4eaefde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 介绍\n",
    "## Transformer 模型包括 编码器，解码器和注意力机制所组成\n",
    "## 相关子模型又分为：编码器模型，解码器模型以及编解码器模型\n",
    "# Model Head 介绍\n",
    "## Model Head 是模型头。连接在模型尾部；\n",
    "## 通常为一个或多个全连接层所组成；\n",
    "## Model Head的作用是将模型输出映射到目标空间，以解决不同类型的任务；\n",
    "\n",
    "# Transformers中的Model head\n",
    "## *Model\n",
    "## *ForCausaLM\n",
    "## *ForMaskedLM\n",
    "## *ForMultipleChoice\n",
    "## *ForQuestionAnswering\n",
    "## *ForSequenceClassification\n",
    "## 等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b410b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Transformer 库中 Model 的基本使用\n",
    "## 模型的加载和保存\n",
    "#### 在线加载、模型下载、离线加载、模型加载参数\n",
    "## 模型调用\n",
    "#### 不带model head的模型调用、带model head的模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "421579fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 模型在家与保存\n",
    "from transformers import AutoConfig,AutoModel,AutoTokenizer\n",
    "## 在线加载\n",
    "model =AutoModel.from_pretrained(\"hfl/rbt3\",force_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08fac873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: `git lfs clone` is deprecated and will not be updated\n",
      "          with new flags from `git clone`\n",
      "\n",
      "`git clone` has been updated in upstream Git to have comparable\n",
      "speeds to `git lfs clone`.\n",
      "Cloning into 'rbt3'...\n",
      "remote: Enumerating objects: 48, done.\u001b[K\n",
      "remote: Total 48 (delta 0), reused 0 (delta 0), pack-reused 48 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (48/48), 156.45 KiB | 459.00 KiB/s, done.\n",
      "Downloading LFS objects: 100% (1/1), 156 MB | 2.3 MB/s                          \r"
     ]
    }
   ],
   "source": [
    "## 模型下载\n",
    "# !git clone  \"https://huggingface.co/hfl/rbt3\"\n",
    "# ## 模型下载，只下载pytorch模型文件\n",
    "!git lfs clone  \"https://huggingface.co/hfl/rbt3\" --include=\"*.bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a5156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 离线加载\n",
    "model = AutoModel.from_pretrained(\"./rbt3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d02e4249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.54.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "255938bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.54.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"./rbt3/\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd2d8d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.output_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0e6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101,  791, 1921, 1921, 3698,  679, 7231,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.5381,  0.8611,  0.5752,  ..., -0.0028,  0.1870, -0.0446],\n",
      "         [-0.2555,  0.7108,  0.6348,  ...,  0.2265, -0.5309,  0.2981],\n",
      "         [ 0.1973,  0.6017, -0.2902,  ..., -0.4512,  0.2120,  0.2460],\n",
      "         ...,\n",
      "         [-0.0547, -0.0721,  0.5215,  ...,  0.0606, -0.5641,  0.1690],\n",
      "         [ 0.3448,  0.3575, -0.1804,  ..., -0.0186, -0.0483,  0.0743],\n",
      "         [ 0.5346,  0.8673,  0.5752,  ...,  0.0040,  0.1896, -0.0417]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 3.2925e-01, -9.9947e-01, -9.9910e-01, -7.4848e-01,  7.8952e-01,\n",
      "          8.0631e-02,  2.1036e-01,  3.4571e-01,  9.9290e-01,  9.9942e-01,\n",
      "          1.2075e-01, -9.9996e-01, -5.8432e-02,  9.9893e-01, -9.9981e-01,\n",
      "          9.9986e-01,  9.8823e-01,  9.7216e-01, -9.3848e-01,  1.1308e-01,\n",
      "         -9.9406e-01, -5.5963e-01,  2.2026e-01,  9.1487e-01,  9.8692e-01,\n",
      "         -9.7932e-01, -9.9839e-01, -1.0082e-01, -7.3547e-01, -9.9893e-01,\n",
      "         -9.6162e-01, -9.9626e-01,  9.7957e-02,  1.6103e-01,  9.9199e-01,\n",
      "         -9.6982e-01, -4.0056e-02, -9.8490e-01, -9.5943e-01, -9.9689e-01,\n",
      "         -6.6652e-02,  9.7813e-01, -1.2231e-01,  9.9874e-01,  3.6309e-01,\n",
      "         -1.6844e-01,  9.9987e-01,  9.9188e-01, -2.5861e-01, -9.4485e-01,\n",
      "         -8.1917e-02,  1.9708e-01, -7.5377e-01,  9.7950e-01,  2.5794e-01,\n",
      "          1.7290e-01,  9.9124e-01, -9.9893e-01, -9.9843e-01,  9.8078e-01,\n",
      "         -9.9751e-01,  9.9716e-01,  9.5322e-01,  9.6599e-01, -9.2341e-01,\n",
      "          9.9911e-01,  9.9499e-01, -3.6721e-01, -4.4337e-01, -9.9961e-01,\n",
      "         -6.1486e-01, -9.1388e-01, -9.9795e-01, -3.2405e-01, -2.9781e-03,\n",
      "         -8.4415e-01,  9.9930e-01, -1.4284e-01,  9.9879e-01, -5.0419e-03,\n",
      "         -9.9565e-01,  3.7877e-03,  1.0697e-01,  3.1637e-02,  9.8930e-01,\n",
      "          9.9996e-01,  3.2775e-01, -9.9637e-01, -4.4373e-01, -9.9818e-01,\n",
      "         -3.2211e-02,  9.9762e-01,  9.9990e-01, -9.9983e-01,  9.9995e-01,\n",
      "         -1.2470e-01,  2.7247e-01,  2.4270e-01, -8.9430e-01,  7.5374e-01,\n",
      "         -4.6553e-03, -1.5926e-01,  9.9814e-01,  8.9413e-01,  7.9639e-01,\n",
      "         -9.9820e-01, -9.6749e-01,  9.9943e-01, -9.9936e-01, -1.0960e-01,\n",
      "          1.0000e+00, -5.7954e-02,  9.9887e-01,  9.9934e-01,  9.8970e-01,\n",
      "         -9.9751e-01,  1.5492e-01,  1.5276e-01, -9.9966e-01,  9.8586e-01,\n",
      "         -9.6425e-01,  3.5314e-01, -2.1805e-01, -5.7051e-02,  4.2188e-02,\n",
      "         -9.9991e-01,  2.1156e-01,  4.3369e-01, -4.5718e-01, -9.7533e-01,\n",
      "         -9.9909e-01, -9.9968e-01,  4.9142e-01,  9.7906e-01, -6.9730e-01,\n",
      "          3.1666e-01,  2.6051e-01, -1.3972e-01, -9.9997e-01, -9.9565e-01,\n",
      "         -9.9983e-01,  8.4057e-01, -7.9816e-01,  9.9162e-01, -9.9707e-01,\n",
      "          9.9968e-01, -9.9637e-01,  9.9919e-01,  9.0446e-01, -3.8871e-01,\n",
      "         -4.0672e-01,  1.3652e-01, -8.5027e-01,  3.1975e-02,  9.1830e-02,\n",
      "          9.3525e-01,  9.2741e-01,  4.6198e-01, -8.3312e-01,  9.9974e-01,\n",
      "         -7.2829e-01,  9.5889e-01,  2.7505e-01,  9.9746e-01,  9.9999e-01,\n",
      "         -9.9942e-01, -3.1416e-02, -9.9999e-01,  6.7349e-01, -7.5296e-03,\n",
      "          9.9974e-01,  9.9945e-01,  5.0103e-01,  9.9249e-01, -6.8391e-01,\n",
      "         -9.9840e-01,  6.5575e-01, -9.9768e-01,  6.8623e-01,  9.9995e-01,\n",
      "         -1.9099e-01,  4.2733e-01,  9.9999e-01,  9.3886e-01,  9.0752e-01,\n",
      "         -4.1577e-01, -3.7151e-02, -9.9325e-01,  4.4489e-01,  8.3798e-01,\n",
      "          9.9693e-01, -9.9296e-01,  1.1970e-02,  9.8971e-01,  1.4966e-01,\n",
      "         -1.4904e-01, -5.3286e-01, -9.2013e-01,  9.9981e-01,  9.8222e-01,\n",
      "         -6.8653e-01,  2.0773e-01,  9.9932e-01,  1.6226e-01,  9.9818e-01,\n",
      "          1.4073e-01,  4.9610e-01, -3.7616e-01,  9.9866e-01,  1.1899e-01,\n",
      "          5.6587e-01,  1.5104e-02,  9.9950e-01, -8.0639e-01, -9.9706e-01,\n",
      "         -2.6816e-02,  6.7680e-01,  2.4168e-01, -8.5282e-01, -7.5409e-01,\n",
      "          1.4915e-03,  9.9604e-01,  2.5595e-01, -9.7815e-01,  9.9861e-01,\n",
      "         -9.8279e-01, -1.7857e-01, -9.9995e-01, -9.6777e-01,  9.9977e-01,\n",
      "         -6.2912e-02, -9.9989e-01,  9.8121e-02,  9.9996e-01, -8.9488e-01,\n",
      "          9.9926e-01,  4.0489e-02, -9.9893e-01,  1.4592e-01, -3.3113e-01,\n",
      "         -9.9998e-01, -9.8500e-01, -9.9999e-01, -2.0978e-01,  1.4390e-01,\n",
      "          9.9348e-01, -1.0000e+00,  3.3730e-02, -9.9996e-01,  9.9053e-01,\n",
      "          9.9987e-01,  7.0583e-02,  1.8793e-01,  9.9995e-01,  7.3745e-01,\n",
      "         -3.9849e-02,  1.6039e-01,  9.6826e-02,  2.9090e-01,  9.5458e-01,\n",
      "          1.0050e-01,  9.9958e-01, -9.9776e-01,  5.5464e-01, -2.4918e-01,\n",
      "         -9.9872e-01,  1.4060e-01,  9.2088e-01,  2.5559e-01, -2.6684e-03,\n",
      "         -1.3990e-01, -4.4207e-02,  9.3571e-01,  9.8776e-01,  1.0000e+00,\n",
      "          8.9939e-01,  9.9996e-01,  9.9996e-01, -3.5792e-02, -9.5786e-01,\n",
      "          5.2542e-01,  2.1941e-01, -9.9968e-01, -9.9753e-01, -9.8976e-01,\n",
      "          6.9237e-01,  2.6122e-02,  1.0000e+00, -9.9997e-01,  9.9994e-01,\n",
      "         -6.3660e-01,  2.7002e-01,  2.8671e-02, -3.4613e-01, -8.9962e-01,\n",
      "         -6.9616e-02,  2.3269e-01,  1.2665e-01,  8.7148e-01,  9.8944e-01,\n",
      "         -7.3227e-02, -1.0463e-01,  6.8557e-01,  9.4185e-02,  7.2550e-01,\n",
      "         -9.3628e-01,  8.2866e-01,  3.5098e-01,  6.6178e-01, -3.9450e-01,\n",
      "         -9.9992e-01,  9.9425e-01, -9.9545e-01, -1.5722e-01, -9.9714e-01,\n",
      "         -9.8189e-01, -3.4940e-04, -9.9583e-01,  9.7253e-01,  9.9174e-01,\n",
      "         -1.9570e-01, -1.0429e-01, -9.9993e-01,  9.4875e-03,  9.9842e-01,\n",
      "          9.9731e-01, -9.9998e-01,  9.8510e-01,  9.2475e-01, -9.0834e-01,\n",
      "          3.8330e-01,  9.7609e-01, -9.9017e-01,  9.9575e-01, -9.9989e-01,\n",
      "         -2.0906e-01,  9.9714e-01,  1.8979e-01, -9.9994e-01, -8.8064e-01,\n",
      "         -1.4281e-01, -5.9439e-01, -3.1277e-01,  9.9960e-01, -3.9077e-01,\n",
      "         -5.1050e-01, -9.9983e-01, -6.1372e-01, -5.5056e-01, -2.6914e-01,\n",
      "          9.9519e-01, -9.9939e-01,  6.7701e-01,  9.8351e-01, -9.9480e-01,\n",
      "         -3.7456e-01,  4.7683e-02,  7.0464e-02, -3.0793e-02, -9.7209e-01,\n",
      "         -9.9627e-01, -9.9793e-01,  9.9727e-01, -4.3451e-01, -9.8163e-02,\n",
      "          9.9441e-01,  9.9986e-01,  9.9808e-01, -9.5643e-01, -1.4084e-01,\n",
      "          9.9339e-01, -1.0509e-01, -3.1251e-01, -9.5298e-01,  2.0148e-01,\n",
      "         -9.0860e-01, -8.6104e-01,  2.1135e-01,  5.3878e-01,  7.0907e-01,\n",
      "         -9.8859e-01,  9.9993e-01,  9.9100e-01,  9.9998e-01,  1.2643e-02,\n",
      "         -6.9391e-01,  3.9559e-01, -5.3023e-01, -9.9535e-01, -6.3144e-02,\n",
      "          9.9953e-01, -9.9373e-01,  3.6758e-01,  6.7256e-01, -9.6629e-01,\n",
      "          9.9982e-01, -9.3191e-01,  1.6875e-01, -1.0000e+00, -9.9684e-01,\n",
      "         -1.0000e+00,  9.9994e-01,  1.9345e-02,  4.7309e-01, -8.5274e-01,\n",
      "          9.9973e-01,  8.8106e-01, -9.0919e-01,  1.1071e-01,  9.9929e-01,\n",
      "         -3.8813e-01, -3.1584e-01, -9.9999e-01, -9.8769e-01,  9.8901e-01,\n",
      "          2.0368e-01,  9.9212e-01,  9.3328e-01, -9.9047e-01,  9.4654e-01,\n",
      "          9.7205e-01, -2.9391e-01, -9.7214e-01, -9.8759e-01,  2.0045e-01,\n",
      "          2.3894e-01, -2.4365e-01, -1.2379e-02, -9.6075e-02,  9.9917e-01,\n",
      "         -8.1827e-02, -2.0458e-01,  2.0950e-01,  9.9987e-01,  9.3174e-01,\n",
      "         -9.9696e-01, -5.7391e-01,  1.1009e-01, -9.4832e-01, -9.9952e-01,\n",
      "         -9.9737e-01,  4.7517e-02, -2.5207e-01,  7.4034e-03, -9.4139e-01,\n",
      "         -9.9972e-01, -9.9503e-01, -1.7009e-01, -9.9265e-01, -9.7794e-01,\n",
      "          1.7375e-01, -9.9996e-01, -9.6914e-01,  9.9666e-01, -9.9978e-01,\n",
      "          2.4949e-01,  9.2915e-01,  3.7270e-01, -9.9988e-01, -3.1246e-01,\n",
      "          9.5974e-01, -9.9631e-01,  1.7894e-01, -9.3164e-01,  2.3580e-01,\n",
      "         -7.7738e-01, -9.9994e-01,  3.6008e-02,  9.9981e-01,  9.9101e-01,\n",
      "          9.9870e-01,  9.5747e-01, -6.3773e-01,  3.7929e-01,  9.9928e-01,\n",
      "          9.9922e-01, -3.6331e-01, -2.2001e-01, -1.0000e+00, -5.0656e-01,\n",
      "          3.8518e-01,  1.9199e-01,  6.7234e-01, -9.9899e-01, -2.9636e-01,\n",
      "         -7.6440e-01,  2.8794e-02,  1.0000e+00,  9.8890e-01,  6.5709e-01,\n",
      "         -9.9999e-01,  8.0577e-02, -6.0495e-01, -4.3124e-01, -7.7467e-01,\n",
      "          2.3219e-01,  9.9984e-01, -9.7632e-01, -1.4046e-01, -9.0917e-01,\n",
      "         -9.9853e-01,  9.9998e-01, -9.9996e-01,  9.9950e-01,  3.4907e-01,\n",
      "         -9.4100e-01,  1.2452e-01, -7.6267e-01,  4.2414e-01, -8.3979e-02,\n",
      "         -2.4396e-01,  2.5094e-02,  2.2205e-01, -9.9967e-01,  4.9247e-01,\n",
      "          9.9836e-01,  6.8945e-02, -9.3191e-01, -9.9949e-01,  7.2734e-02,\n",
      "          9.9389e-01, -9.8992e-01, -9.9974e-01, -3.5339e-01, -5.6459e-01,\n",
      "         -1.7556e-01,  1.8285e-01, -5.8320e-02,  5.3663e-01, -8.4955e-01,\n",
      "          3.1291e-01,  8.4845e-01, -9.6819e-02,  8.3539e-01, -4.6805e-01,\n",
      "         -9.5600e-01, -1.9171e-01,  9.9925e-01,  9.8933e-01, -9.9984e-01,\n",
      "         -9.9990e-01, -7.0674e-01, -2.5338e-02, -9.2553e-01,  9.9412e-01,\n",
      "         -4.4884e-01, -9.6005e-01,  1.8153e-02,  1.3971e-01, -1.9934e-01,\n",
      "         -8.3875e-01,  9.9998e-01, -9.8354e-01,  9.9999e-01, -9.9999e-01,\n",
      "         -9.9824e-01,  2.2434e-01,  9.9976e-01, -9.9980e-01, -6.4725e-02,\n",
      "          9.9892e-01, -9.9990e-01, -2.7200e-01, -5.5264e-01,  4.3501e-01,\n",
      "         -3.3425e-01, -5.2192e-02,  9.4093e-01, -9.7814e-01,  1.3489e-01,\n",
      "         -9.9756e-01,  9.6248e-01,  9.2199e-01, -9.9792e-01,  3.9467e-01,\n",
      "         -9.9999e-01,  2.1280e-03, -1.2792e-01, -9.9991e-01,  9.9326e-01,\n",
      "          9.9877e-01, -2.9909e-01, -6.6711e-01, -9.9853e-01, -3.8636e-01,\n",
      "          1.0213e-01, -9.9934e-01,  4.6531e-01, -9.9974e-01,  8.2246e-02,\n",
      "         -9.8532e-01,  9.9451e-01, -9.9780e-01,  9.7043e-01,  8.8893e-01,\n",
      "         -7.8909e-02, -9.6421e-01, -2.1948e-01, -4.5554e-01, -9.9931e-01,\n",
      "          1.7511e-01, -9.9839e-01, -9.9318e-01,  7.8151e-04,  9.9816e-01,\n",
      "          9.5852e-01,  1.6418e-01,  9.9691e-01, -9.4257e-01, -3.4481e-01,\n",
      "         -4.7047e-01,  7.2419e-01,  1.0000e+00, -9.9967e-01, -9.9979e-01,\n",
      "          9.5765e-01, -9.9990e-01, -9.8137e-01,  1.0000e+00, -1.6585e-01,\n",
      "          9.9995e-01, -7.5326e-02, -9.8075e-01,  1.6000e-01,  7.5654e-02,\n",
      "          9.9898e-01, -1.2700e-01, -1.7890e-01,  6.9788e-01,  1.2697e-01,\n",
      "          4.7121e-01, -3.4895e-01,  9.3701e-01,  5.5316e-02, -2.0496e-01,\n",
      "          6.5571e-01, -9.9090e-01, -9.9300e-01, -9.8996e-01,  3.3041e-01,\n",
      "         -8.3728e-02,  2.9383e-01,  2.7800e-01,  9.1101e-01,  9.8872e-01,\n",
      "         -9.8238e-01,  9.7883e-01, -1.0000e+00, -9.9982e-01, -3.3111e-01,\n",
      "          3.5535e-01,  8.1795e-01,  2.9988e-01, -3.1320e-01, -1.4697e-01,\n",
      "         -9.9842e-01,  9.0054e-01, -9.5702e-01,  9.8913e-01, -1.0380e-01,\n",
      "          2.2498e-01,  9.9998e-01,  9.9485e-01,  7.2250e-02, -9.9369e-01,\n",
      "         -9.3468e-01,  1.1675e-01, -9.9084e-01,  9.9112e-01, -2.5430e-01,\n",
      "         -4.2083e-01,  4.0064e-01,  2.9969e-02, -9.9301e-01, -9.7339e-01,\n",
      "          2.2422e-01,  9.9504e-01, -9.9262e-01,  7.7778e-01, -9.3778e-01,\n",
      "          9.5256e-01,  9.9450e-01,  1.0000e+00, -7.9412e-02,  9.3193e-01,\n",
      "         -9.8700e-01, -9.9629e-01,  9.9160e-01,  9.9557e-01,  9.9998e-01,\n",
      "          9.5906e-01,  7.0611e-01,  3.7098e-01, -9.9976e-01,  9.9284e-01,\n",
      "         -1.6102e-01,  2.7583e-01,  3.2868e-01, -9.7802e-01, -9.9986e-01,\n",
      "          9.9998e-01, -1.0000e+00, -9.9687e-01, -3.8391e-01, -9.9664e-01,\n",
      "          9.9487e-01,  8.1223e-01,  9.5909e-01, -3.9637e-01, -9.9662e-01,\n",
      "         -9.7858e-01,  1.9806e-01, -9.7110e-01, -6.7144e-01, -3.9286e-01,\n",
      "         -9.9997e-01,  2.9793e-01,  1.8111e-01, -9.9625e-01, -1.8013e-01,\n",
      "         -7.9614e-01,  6.7606e-01,  8.4881e-01, -9.6815e-01,  5.7416e-01,\n",
      "         -9.7485e-01, -9.9848e-01,  4.6578e-03, -9.9999e-01,  9.7385e-01,\n",
      "          9.9933e-01, -1.9271e-01, -4.3549e-01, -4.5701e-01,  4.7717e-01,\n",
      "         -9.9992e-01, -9.9968e-01,  9.7617e-01,  9.9994e-01,  6.6071e-02,\n",
      "         -9.8334e-01, -3.9048e-01, -9.9345e-01, -1.9977e-01,  8.8154e-01,\n",
      "          9.9164e-01, -9.9932e-01,  9.8213e-01, -6.6668e-01,  5.4827e-02,\n",
      "          9.9434e-01, -1.0000e+00,  9.4527e-01, -9.9985e-01,  9.9754e-01,\n",
      "         -9.9982e-01,  9.9973e-01,  1.1709e-01,  4.2808e-01, -9.2866e-02,\n",
      "          8.9940e-01, -9.9955e-01, -5.6015e-01,  7.8899e-01,  9.9745e-01,\n",
      "          8.5058e-02,  9.1215e-01,  1.9476e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuhaoliu/micromamba/envs/transformers/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 2 模型调用\n",
    "## 不带model head的模型调用\n",
    "\n",
    "sen = \"今天天气不错\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./rbt3\")\n",
    "inputs = tokenizer(sen,return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "model = AutoModel.from_pretrained(\"./rbt3\")\n",
    "outputs = model(**tokenizer(sen,return_tensors=\"pt\"))\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06c4040f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5381,  0.8611,  0.5752,  ..., -0.0028,  0.1870, -0.0446],\n",
       "         [-0.2555,  0.7108,  0.6348,  ...,  0.2265, -0.5309,  0.2981],\n",
       "         [ 0.1973,  0.6017, -0.2902,  ..., -0.4512,  0.2120,  0.2460],\n",
       "         ...,\n",
       "         [-0.0547, -0.0721,  0.5215,  ...,  0.0606, -0.5641,  0.1690],\n",
       "         [ 0.3448,  0.3575, -0.1804,  ..., -0.0186, -0.0483,  0.0743],\n",
       "         [ 0.5346,  0.8673,  0.5752,  ...,  0.0040,  0.1896, -0.0417]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5178e397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34371a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3db0f4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## 带 model head 的模型调用\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "clz_model = AutoModelForSequenceClassification.from_pretrained(\"rbt3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65ca7ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuhaoliu/micromamba/envs/transformers/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.4401, -0.4447]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clz_model(**tokenizer(sen,return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec33857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
