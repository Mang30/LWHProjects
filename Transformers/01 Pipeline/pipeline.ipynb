{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7ad01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline的使用\n",
    "# pipeline 是一种设计方式，将模型的输入、输出、预处理、后处理等步骤封装在一起，方便使用。\n",
    "\n",
    "from transformers.pipelines import SUPPORTED_TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb824fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4b34ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-classification audio\n",
      "automatic-speech-recognition multimodal\n",
      "text-to-audio text\n",
      "feature-extraction multimodal\n",
      "text-classification text\n",
      "token-classification text\n",
      "question-answering text\n",
      "table-question-answering text\n",
      "visual-question-answering multimodal\n",
      "document-question-answering multimodal\n",
      "fill-mask text\n",
      "summarization text\n",
      "translation text\n",
      "text2text-generation text\n",
      "text-generation text\n",
      "zero-shot-classification text\n",
      "zero-shot-image-classification multimodal\n",
      "zero-shot-audio-classification multimodal\n",
      "image-classification image\n",
      "image-feature-extraction image\n",
      "image-segmentation multimodal\n",
      "image-to-text multimodal\n",
      "image-text-to-text multimodal\n",
      "object-detection multimodal\n",
      "zero-shot-object-detection multimodal\n",
      "depth-estimation image\n",
      "video-classification video\n",
      "mask-generation multimodal\n",
      "image-to-image image\n"
     ]
    }
   ],
   "source": [
    "for k,v in SUPPORTED_TASKS.items():\n",
    "    print(k,v[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9985874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline的创建和使用方式\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d151d828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.999830961227417}]\n"
     ]
    }
   ],
   "source": [
    "# 1. 根据任务类型直接创建Pipeline，默认都是英文模型\n",
    "pipe = pipeline(\"text-classification\")\n",
    "\n",
    "result = pipe(\"like you\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58e4f7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive (stars 4 and 5)', 'score': 0.9009531140327454}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuhaoliu/micromamba/envs/transformers/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 2. 根据任务类型和模型名称创建Pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "\n",
    "result = pipe(\"I love you\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e39796ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative (stars 1, 2 and 3)', 'score': 0.9818915724754333}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuhaoliu/micromamba/envs/transformers/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "result = pipe(\"今天天气真差劲呀！\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c7ad8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'negative (stars 1, 2 and 3)', 'score': 0.9818915724754333}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuhaoliu/micromamba/envs/transformers/lib/python3.9/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 3. 预先加载模型和分词器，再创建 pipeline\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "\n",
    "pipe=pipeline(\"text-classification\",model=model,tokenizer=tokenizer)\n",
    "\n",
    "result=pipe(\"今天天气真差劲呀！\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d610e9c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe\u001b[38;5;241m=\u001b[39m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m result\u001b[38;5;241m=\u001b[39mpipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m今天天气真差劲呀！\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/micromamba/envs/transformers/lib/python3.9/site-packages/transformers/pipelines/__init__.py:1059\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m load_tokenizer:\n\u001b[0;32m-> 1059\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1061\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/transformers/lib/python3.9/site-packages/transformers/pipelines/__init__.py:1037\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1034\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m         \u001b[38;5;66;03m# Impossible to guess what is the right tokenizer here\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m   1038\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImpossible to guess which tokenizer to use. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1039\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1040\u001b[0m         )\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# Instantiate tokenizer if needed\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokenizer, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[0;31mException\u001b[0m: Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer."
     ]
    }
   ],
   "source": [
    "pipe=pipeline(\"text-classification\",model=model)\n",
    "\n",
    "result=pipe(\"今天天气真差劲呀！\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af35493b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a03be9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\",model=\"uer/roberta-base-finetuned-dianping-chinese\",device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5da27fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04808483839035034\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "times = []\n",
    "for i in range(100):\n",
    "    torch.mps.synchronize()\n",
    "    start = time.time()\n",
    "    pipe(\"今天天气真差劲呀！\")\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "\n",
    "\n",
    "print(sum(times)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dcd57ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\",model=\"uer/roberta-base-finetuned-dianping-chinese\",device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f51e0ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011895489692687989\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "times = []\n",
    "for i in range(100):\n",
    "    torch.mps.synchronize()\n",
    "    start = time.time()\n",
    "    pipe(\"今天天气真差劲呀！\")\n",
    "    end = time.time()\n",
    "    times.append(end - start)\n",
    "\n",
    "\n",
    "print(sum(times)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f25c5e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x11eb09370>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 使用pipeline的batch参数\n",
    "pipe = pipeline(\"text-classification\",model=\"uer/roberta-base-finetuned-dianping-chinese\",batch_size=10)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3733b7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mTextClassificationPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Text classification pipeline using any `ModelForSequenceClassification`. See the [sequence classification\n",
      "examples](../task_summary#sequence-classification) for more information.\n",
      "\n",
      "Example:\n",
      "\n",
      "```python\n",
      ">>> from transformers import pipeline\n",
      "\n",
      ">>> classifier = pipeline(model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
      ">>> classifier(\"This movie is disgustingly good !\")\n",
      "[{'label': 'POSITIVE', 'score': 1.0}]\n",
      "\n",
      ">>> classifier(\"Director tried too much.\")\n",
      "[{'label': 'NEGATIVE', 'score': 0.996}]\n",
      "```\n",
      "\n",
      "Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
      "\n",
      "This text classification pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      "`\"sentiment-analysis\"` (for classifying sequences according to positive or negative sentiments).\n",
      "\n",
      "If multiple classification labels are available (`model.config.num_labels >= 2`), the pipeline will run a softmax\n",
      "over the results. If there is a single label, the pipeline will run a sigmoid over the result. In case of regression\n",
      "tasks (`model.config.problem_type == \"regression\"`), will not apply any function on the output.\n",
      "\n",
      "The models that this pipeline can use are models that have been fine-tuned on a sequence classification task. See\n",
      "the up-to-date list of available models on\n",
      "[huggingface.co/models](https://huggingface.co/models?filter=text-classification).\n",
      "\n",
      "Arguments:\n",
      "    model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      "        The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      "        [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      "    tokenizer ([`PreTrainedTokenizer`]):\n",
      "        The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      "        [`PreTrainedTokenizer`].\n",
      "    modelcard (`str` or [`ModelCard`], *optional*):\n",
      "        Model card attributed to the model for this pipeline.\n",
      "    framework (`str`, *optional*):\n",
      "        The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      "        installed.\n",
      "\n",
      "        If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      "        both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      "        provided.\n",
      "    task (`str`, defaults to `\"\"`):\n",
      "        A task-identifier for the pipeline.\n",
      "    num_workers (`int`, *optional*, defaults to 8):\n",
      "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      "        workers to be used.\n",
      "    batch_size (`int`, *optional*, defaults to 1):\n",
      "        When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      "        the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      "        pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      "    args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      "        Reference to the object in charge of parsing supplied pipeline parameters.\n",
      "    device (`int`, *optional*, defaults to -1):\n",
      "        Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      "        the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
      "    torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      "        Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      "        (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
      "    binary_output (`bool`, *optional*, defaults to `False`):\n",
      "        Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
      "        the raw output data e.g. text.\n",
      "    return_all_scores (`bool`, *optional*, defaults to `False`):\n",
      "        Whether to return all prediction scores or just the one of the predicted class.\n",
      "    function_to_apply (`str`, *optional*, defaults to `\"default\"`):\n",
      "        The function to apply to the model outputs in order to retrieve the scores. Accepts four different values:\n",
      "\n",
      "        - `\"default\"`: if the model has a single label, will apply the sigmoid function on the output. If the model\n",
      "          has several labels, will apply the softmax function on the output. In case of regression tasks, will not\n",
      "          apply any function on the output.\n",
      "        - `\"sigmoid\"`: Applies the sigmoid function on the output.\n",
      "        - `\"softmax\"`: Applies the softmax function on the output.\n",
      "        - `\"none\"`: Does not apply any function on the output.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/micromamba/envs/transformers/lib/python3.9/site-packages/transformers/pipelines/text_classification.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "TextClassificationPipeline?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe058d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其他 pipeline 展示\n",
    "checkpoint = \"google/owlvit-base-patch32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60130164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline 的背后实现\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16a08fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  791, 1921, 1921, 3698, 4696, 2345, 1226, 1435, 8013,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"今天天气真差劲呀！\"\n",
    "inputs = tokenizer(input_text,return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca1a879d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.9391, -2.0540]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(**inputs) # 利用字典解包的方式，将输入的参数传给模型\n",
    "# res = model(input_ids=inputs[\"input_ids\"], token_type_ids=inputs[\"token_type_ids\"], attention_mask=inputs[\"attention_mask\"] )\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf7a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9819, 0.0181]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = res.logits # 提取模型输出的 logits 结果\n",
    "logits = torch.softmax(logits, dim=-1) # 将模型输出的 logits 结果给归一化\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "482e1d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.argmax(logits).item() # # 取概率最大的类别索引，并转为Python整数\n",
    "pred # 显示预测的类别编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "78da47ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"negative (stars 1, 2 and 3)\",\n",
       "    \"1\": \"positive (stars 4 and 5)\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"negative (stars 1, 2 and 3)\": 0,\n",
       "    \"positive (stars 4 and 5)\": 1\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.54.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "760f28f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'negative (stars 1, 2 and 3)', 1: 'positive (stars 4 and 5)'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a8ea86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative (stars 1, 2 and 3)'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.config.id2label.get(pred) # 使用.get()字典函数安全取值\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "02325ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101,  791, 1921, 1921, 3698, 4696, 2345, 1226, 1435, 8013,  102]])\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(inputs.get(\"input_ids\"))\n",
    "print()\n",
    "print(inputs.get(\"input_is\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c76c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
